[["index.html", "STM Chapter 1 Editing Notes", " STM Jilung Hsieh 2022-04-26 Chapter 1 Editing Notes This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["introduction.html", "Chapter 2 Introduction 2.1 Diagram testing", " Chapter 2 Introduction knitr::opts_chunk$set(echo = TRUE, cache = T, message=FALSE, warning=FALSE, class.output = &quot;output&quot;) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.6 ✓ dplyr 1.0.8 ## ✓ tidyr 1.2.0 ✓ stringr 1.4.0 ## ✓ readr 2.1.2 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() tb &lt;- tibble(a=c(1, 2, 3), b=c(4, 5, 6)) tb ## # A tibble: 3 × 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 ## 2 2 5 ## 3 3 6 2.1 Diagram testing https://graphviz.org/doc/info/shapes.html DiagrammeR::grViz(&quot; digraph G { graph [layout = dot] node [shape = folder, style = filled, margin=0, height=0.1] node [fillcolor = lightblue] a [label = &#39;111111&#39;] node [fillcolor = Beige] b [label = &#39;2222&#39;] c [label = &#39;333333&#39;] d [label = &#39;444&#39;] node [fillcolor = mediumpurple1] e [label = &#39;5&#39;] f [label = &#39;6&#39;] g [label = &#39;7&#39;] node [fillcolor = darkseagreen1] h [label = &#39;8&#39;] i [label = &#39;9&#39;] subgraph cluster1 { # node [fixedsize = true, width = 3] b -&gt; {c d} } subgraph cluster2 { # node [fixedsize = true, width = 3] e -&gt; f -&gt; g -&gt; e } subgraph cluster3 { # node [fixedsize = true, width = 3] h i } a -&gt; b a -&gt; e f -&gt; i e -&gt; h } &quot;) "],["copus.html", "Chapter 3 Corpus", " Chapter 3 Corpus knitr::opts_chunk$set(echo = TRUE, cache = T, class.output = &quot;output&quot;) "],["tokenization.html", "Chapter 4 Tokenization", " Chapter 4 Tokenization knitr::opts_chunk$set(echo = TRUE, cache = T, class.output = &quot;output&quot;) "],["keyness.html", "Chapter 5 Keyness", " Chapter 5 Keyness knitr::opts_chunk$set(echo = TRUE, cache = T, class.output = &quot;output&quot;) "],["collocation.html", "Chapter 6 Collocation", " Chapter 6 Collocation knitr::opts_chunk$set(echo = TRUE, cache = T, class.output = &quot;output&quot;) "],["embedding.html", "Chapter 7 Embedding", " Chapter 7 Embedding "],["topicmodel.html", "Chapter 8 Topic Model 8.1 STM", " Chapter 8 Topic Model 8.1 STM Reference: https://juliasilge.com/blog/evaluating-stm/ - https://juliasilge.com/blog/sherlock-holmes-stm/ - RPubs - stm_course 8.1.1 Setups pkgs &lt;- c(&quot;LDAvis&quot;, &quot;tidyverse&quot;, &quot;jiebaR&quot;, &quot;igraph&quot;, &quot;stm&quot;, &quot;wordcloud&quot;) install.packages(pkgs[!pkgs %in% installed.packages()]) library(tidyverse) library(tidyr) options(scipen = 999) load(&quot;data/s3_watched.rda&quot;) Sys.setlocale(locale=&quot;zh_TW.UTF-8&quot;) ## [1] &quot;zh_TW.UTF-8/zh_TW.UTF-8/zh_TW.UTF-8/C/zh_TW.UTF-8/zh_TW.UTF-8&quot; library(jiebaR) stopWords &lt;- readRDS(&quot;data/stopWords.rds&quot;) segment_not &lt;- c(&quot;爸爸&quot;, &quot;爸媽&quot;, &quot;新手&quot;) watched &lt;- c(&quot;爸爸&quot;,&quot;父親&quot;,&quot;老公&quot;,&quot;先生&quot;,&quot;丈夫&quot;,&quot;奶爸&quot;,&quot;寶爸&quot;,&quot;隊友&quot;, &quot;爹地&quot;,&quot;爸比&quot;,&quot;把拔&quot;,&quot;把鼻&quot;,&quot;老爸&quot;,&quot;另一半&quot;,&quot;拔拔&quot;, &quot;孩子的爸&quot;,&quot;孩子爸&quot;, &quot;爸拔&quot;,&quot;他爸&quot;,&quot;她爸&quot;,&quot;新手爸&quot;,&quot;版爸&quot;, &quot;板爸&quot;,&quot;我家男人&quot;,&quot;當爸的&quot;,&quot;腦公&quot;,&quot;阿爸&quot;,&quot;人父&quot;,&quot;孩子的爹&quot;, &quot;孩子爹&quot;,&quot;老爹&quot;,&quot;外子&quot;,&quot;拔比&quot;,&quot;爸鼻&quot;,&quot;爸把&quot;,&quot;爸逼&quot;,&quot;爸咪&quot;, &quot;把爸&quot;,&quot;拔爸&quot;,&quot;爹低&quot;,&quot;帥爸&quot;,&quot;準爸&quot;,&quot;小孩爸&quot;,&quot;親爸&quot;,&quot;神爸&quot;, &quot;宅爸&quot;,&quot;瓶餵爸&quot;,&quot;寶寶的爸&quot;,&quot;孩的爸&quot;,&quot;女兒的爸&quot;) reserved &lt;- c(&quot;神隊友&quot;, &quot;豬隊友&quot;, &quot;好隊友&quot;, &quot;好先生&quot;, &quot;好爸爸&quot;, &quot;好老公&quot;) watched &lt;- c(watched, reserved) watched.str &lt;- paste0(watched, collapse = &quot;|&quot;) reserved &lt;- c(&quot;神隊友&quot;, &quot;豬隊友&quot;, &quot;好隊友&quot;, &quot;好先生&quot;, &quot;好爸爸&quot;, &quot;好老公&quot;) cutter &lt;- worker() tagger &lt;- worker(&quot;tag&quot;) new_user_word(cutter, segment_not) %&gt;% invisible() new_user_word(cutter, watched) %&gt;% invisible() new_user_word(tagger, segment_not) %&gt;% invisible() new_user_word(tagger, watched) %&gt;% invisible() 8.1.2 Pre-processing unnested.df &lt;- s3.watched %&gt;% filter(str_detect(sentence, &quot;隊友&quot;)) %&gt;% mutate(word = purrr::map(s3, function(x)segment(x, cutter))) %&gt;% unnest(word) %&gt;% anti_join(stopWords) %&gt;% filter(!str_detect(word, &quot;[a-zA-Z0-9]+&quot;)) %&gt;% filter(!is.na(word)) %&gt;% group_by(word) %&gt;% filter(n() &gt; 5) %&gt;% ungroup() %&gt;% filter(nchar(word) &gt; 1) library(tidytext) # library(quanteda) dfm &lt;- unnested.df %&gt;% count(doc_id, word, sort = TRUE) %&gt;% cast_dfm(doc_id, word, n) #tidytext 8.1.3 STM Generated topic model can be saved as rda file for furture use library(stm) topic_model &lt;- stm(dfm, K = 12, verbose = F) # save(topic_model, file=&quot;output/tm02_s3_k12.rda&quot;) # load(&quot;output/tm02_s3_k12.rda&quot;) summary(topic_model) ## A topic model with 12 topics, 2925 documents and a 3072 word dictionary. ## Topic 1 Top Words: ## Highest Prob: 問題, 知道, 媽媽, 發現, 有沒有, 一些, 比較 ## FREX: 提籃, 有沒有, 問題, 開車, 解決, 提供, 類似 ## Lift: 提籃, 爸比, 保證, 很常, 理論, 輪子, 每晚 ## Score: 有沒有, 提籃, 問題, 保證, 發現, 每晚, 知道 ## Topic 2 Top Words: ## Highest Prob: 隊友, 大寶, 晚上, 時間, 上班, 回家, 幫忙 ## FREX: 大寶, 白天, 上班, 中心, 一打, 晚上, 下班 ## Lift: 放學, 清醒, 硬塊, 作月子, 點到, 放電, 加班 ## Score: 大寶, 隊友, 中心, 晚上, 白天, 上班, 擠奶 ## Topic 3 Top Words: ## Highest Prob: 討論, 反應, 重點, 原因, 味道, 困擾, 冰箱 ## FREX: 口味, 附上, 禮盒, 這款, 清潔, 拿來, 親愛 ## Lift: 隔離, 戒菸, 口味, 母湯, 巧克力, 乳腺炎, 晚安 ## Score: 禮盒, 胃口, 討論, 油飯, 警衛, 附上, 這款 ## Topic 4 Top Words: ## Highest Prob: 寶寶, 隊友, 覺得, 一直, 媽媽, 目前, 喜歡 ## FREX: 配方, 寶寶, 請問, 名字, 母乳, 喜歡, 奶粉 ## Lift: 該不該, 好用, 兩款, 兩難, 亂跑, 貓咪, 名單 ## Score: 寶寶, 隊友, 名字, 正向, 推車, 覺得, 意見 ## Topic 5 Top Words: ## Highest Prob: 豬隊友, 女兒, 小孩, 媽媽, 半夜, 餵奶, 嬰兒 ## FREX: 奶嘴, 豬隊友, 餵奶, 女兒, 訓練, 餵食, 換尿布 ## Lift: 起跳, 完奶, 餵食, 搬出來, 代幣, 那次, 奶嘴 ## Score: 豬隊友, 女兒, 男性, 奶嘴, 餵奶, 半夜, 小孩 ## Topic 6 Top Words: ## Highest Prob: 隊友, 兒子, 一個, 今天, 看到, 我們, 最近 ## FREX: 兒子, 使用, 衣服, 過敏, 空氣, 包巾, 消毒 ## Lift: 背心, 補習, 地墊, 蓋子, 喝到, 清淨機, 上會 ## Score: 隊友, 兒子, 提醒, 使用, 跑腿, 尿布, 過敏 ## Topic 7 Top Words: ## Highest Prob: 隊友, 醫生, 我們, 醫師, 診所, 檢查, 一直 ## FREX: 診所, 嫂子, 檢查, 報告, 症狀, 感冒, 傳染 ## Lift: 鼻塞, 不看, 陳醫師, 傳染, 端菜, 甘蔗, 恢復正常 ## Score: 醫生, 醫師, 嫂子, 診所, 隊友, 超音波, 檢查 ## Topic 8 Top Words: ## Highest Prob: 婆婆, 老公, 隊友, 幫忙, 婆家, 公婆, 娘家 ## FREX: 婆婆, 孫子, 坐月子, 老公, 我媽, 公公, 不在 ## Lift: 婆媳, 宴客, 還帶, 很弱, 認生, 孫子, 小嬸 ## Score: 婆婆, 老公, 婆家, 公婆, 公公, 坐月子, 隊友 ## Topic 9 Top Words: ## Highest Prob: 真的, 隊友, 孩子, 覺得, 好隊友, 我們, 謝謝 ## FREX: 憂鬱, 情緒, 好隊友, 先生, 心情, 經濟, 辛苦 ## Lift: 性子, 愛的, 變大, 打拼, 道理, 好難, 好先生 ## Score: 好隊友, 隊友, 真的, 先生, 情緒, 孩子, 老公 ## Topic 10 Top Words: ## Highest Prob: 神隊友, 媽媽, 爸爸, 分享, 一起, 育兒, 希望 ## FREX: 神隊友, 課程, 參與, 運動, 回覆, 育兒, 參考 ## Lift: 北醫, 場次, 大小事, 哥們, 健身房, 日常, 蒐集 ## Score: 神隊友, 爸爸, 育兒, 分享, 原則, 媽媽, 運動 ## Topic 11 Top Words: ## Highest Prob: 隊友, 護理, 醫院, 醫生, 陣痛, 真的, 一直 ## FREX: 陣痛, 無痛, 痛到, 宮縮, 內診, 催生, 傷口 ## Lift: 退貨, 產兆, 單人房, 發明, 縫合, 宮縮, 好痛 ## Score: 陣痛, 無痛, 護理, 醫院, 產房, 隊友, 內診 ## Topic 12 Top Words: ## Highest Prob: 隊友, 小孩, 覺得, 公婆, 直接, 我們, 不想 ## FREX: 小朋友, 小孩, 長輩, 弟弟, 哥哥, 上學, 不爽 ## Lift: 班上, 衝康, 老木, 前天, 他媽, 溫和, 薪資 ## Score: 小孩, 隊友, 前天, 公婆, 小朋友, 保母, 溝通 # print(topic_model) Using wordcloud for visualization often leads to misunderstanding due to the number of letters in the word. # install.packages(&quot;wordcloud&quot;) cloud(topic_model, topic = 7, scale = c(4,.5), family = &quot;Heiti TC Light&quot;) library(igraph) mod.out.corr &lt;- topicCorr(topic_model) plot(mod.out.corr) # mod.out.corr 8.1.4 LDAvis See Sievert, C., &amp; Shirley, K. (2014). LDAvis: A method for visualizing and interpreting topics. Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces. Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, Baltimore, Maryland, USA. https://doi.org/10.3115/v1/w14-3110 saliency(term w) = frequency(w) * [sum_t p(t | w) * log(p(t | w)/p(t))] for topics t; see Chuang et. al (2012) relevance(term w | topic t) = λ * p(w | t) + (1 - λ) * p(w | t)/p(w); see Sievert &amp; Shirley (2014). Lift: p(w|t)/p(w) = p(w and t)/(p(w)p(t)) stm.doc &lt;- quanteda::convert(dfm, to = &quot;stm&quot;) toLDAvis(topic_model, stm.doc$documents) 8.1.5 Validating library(furrr) plan(multiprocess) many_models &lt;- tibble(K = c(8, 16, 24, 32, 64)) %&gt;% mutate(topic_model = future_map(K, ~stm(dfm, K = ., verbose = F))) heldout &lt;- make.heldout(dfm) k_result &lt;- many_models %&gt;% mutate(exclusivity = map(topic_model, exclusivity), semantic_coherence = map(topic_model, semanticCoherence, dfm), eval_heldout = map(topic_model, eval.heldout, heldout$missing), residual = map(topic_model, checkResiduals, dfm), bound = map_dbl(topic_model, function(x) max(x$convergence$bound)), lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)), lbound = bound + lfact, iterations = map_dbl(topic_model, function(x) length(x$convergence$bound))) k_result ## # A tibble: 5 × 10 ## K topic_model exclusivity semantic_coherence eval_heldout residual ## &lt;dbl&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 8 &lt;STM&gt; &lt;dbl [8]&gt; &lt;dbl [8]&gt; &lt;named list [4]&gt; &lt;named list&gt; ## 2 16 &lt;STM&gt; &lt;dbl [16]&gt; &lt;dbl [16]&gt; &lt;named list [4]&gt; &lt;named list&gt; ## 3 24 &lt;STM&gt; &lt;dbl [24]&gt; &lt;dbl [24]&gt; &lt;named list [4]&gt; &lt;named list&gt; ## 4 32 &lt;STM&gt; &lt;dbl [32]&gt; &lt;dbl [32]&gt; &lt;named list [4]&gt; &lt;named list&gt; ## 5 64 &lt;STM&gt; &lt;dbl [64]&gt; &lt;dbl [64]&gt; &lt;named list [4]&gt; &lt;named list&gt; ## # … with 4 more variables: bound &lt;dbl&gt;, lfact &lt;dbl&gt;, lbound &lt;dbl&gt;, ## # iterations &lt;dbl&gt; k_result %&gt;% transmute(K, `Lower bound` = lbound, Residuals = map_dbl(residual, &quot;dispersion&quot;), `Semantic coherence` = map_dbl(semantic_coherence, mean), `Held-out likelihood` = map_dbl(eval_heldout, &quot;expected.heldout&quot;)) %&gt;% gather(Metric, Value, -K) %&gt;% ggplot(aes(K, Value, color = Metric)) + geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) + facet_wrap(~Metric, scales = &quot;free_y&quot;) + labs(x = &quot;K (number of topics)&quot;, y = NULL, title = &quot;Model diagnostics by number of topics&quot;, subtitle = &quot;These diagnostics indicate that a good number of topics would be around 60&quot;) k_result %&gt;% select(K, exclusivity, semantic_coherence) %&gt;% filter(K %in% c(16, 24, 64)) %&gt;% unnest() %&gt;% mutate(K = as.factor(K)) %&gt;% ggplot(aes(semantic_coherence, exclusivity, color = K)) + geom_point(size = 2, alpha = 0.7) + labs(x = &quot;Semantic coherence&quot;, y = &quot;Exclusivity&quot;, title = &quot;Comparing exclusivity and semantic coherence&quot;, subtitle = &quot;Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity&quot;) 8.1.6 Exploring model td_beta &lt;- tidy(topic_model) td_beta %&gt;% filter(topic==1) %&gt;% arrange(-beta) %&gt;% head(10) ## # A tibble: 10 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 問題 0.0626 ## 2 1 知道 0.0534 ## 3 1 媽媽 0.0415 ## 4 1 發現 0.0333 ## 5 1 有沒有 0.0316 ## 6 1 一些 0.0265 ## 7 1 比較 0.0261 ## 8 1 幾天 0.0221 ## 9 1 狀況 0.0214 ## 10 1 有人 0.0197 td_gamma &lt;- tidy(topic_model, matrix = &quot;gamma&quot;, document_names = rownames(dfm)) td_gamma ## # A tibble: 35,100 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 M.1476742346.A.F57 1 0.00281 ## 2 M.1563679368.A.C2B 1 0.0564 ## 3 M.1600923114.A.8F4 1 0.00666 ## 4 M.1488086113.A.85C 1 0.00301 ## 5 M.1533754307.A.070 1 0.0127 ## 6 M.1549687071.A.D3A 1 0.116 ## 7 M.1500566217.A.CE9 1 0.00351 ## 8 M.1554769984.A.A64 1 0.00542 ## 9 M.1593828875.A.DB9 1 0.0616 ## 10 M.1554558794.A.4AA 1 0.00811 ## # … with 35,090 more rows top_terms &lt;- td_beta %&gt;% arrange(beta) %&gt;% group_by(topic) %&gt;% top_n(6, beta) %&gt;% arrange(-beta) %&gt;% select(topic, term) %&gt;% summarise(terms = list(term)) %&gt;% mutate(terms = map(terms, paste, collapse = &quot;, &quot;)) %&gt;% unnest() gamma_terms &lt;- td_gamma %&gt;% group_by(topic) %&gt;% summarise(gamma = mean(gamma)) %&gt;% arrange(desc(gamma)) %&gt;% left_join(top_terms, by = &quot;topic&quot;) %&gt;% mutate(topic = paste0(&quot;Topic &quot;, topic), topic = reorder(topic, gamma)) gamma_terms %&gt;% ggplot(aes(topic, gamma, label = terms, fill = topic)) + geom_col(show.legend = FALSE) + geom_text(hjust = 0, nudge_y = 0.0005, size = 3, family = &quot;Heiti TC Light&quot;) + coord_flip() + scale_y_continuous(expand = c(0,0), limits = c(0, max(gamma_terms$gamma)+0.1), labels = scales::percent_format()) + theme(plot.title = element_text(size = 16, family=&quot;Heiti TC Light&quot;), plot.subtitle = element_text(size = 13)) + theme_minimal() gamma_terms %&gt;% select(topic, gamma, terms) ## # A tibble: 12 × 3 ## topic gamma terms ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Topic 9 0.136 真的, 隊友, 孩子, 覺得, 好隊友, 我們 ## 2 Topic 2 0.122 隊友, 大寶, 晚上, 時間, 上班, 回家 ## 3 Topic 4 0.120 寶寶, 隊友, 覺得, 一直, 媽媽, 目前 ## 4 Topic 6 0.102 隊友, 兒子, 一個, 今天, 看到, 我們 ## 5 Topic 10 0.100 神隊友, 媽媽, 爸爸, 分享, 一起, 育兒 ## 6 Topic 12 0.0902 隊友, 小孩, 覺得, 公婆, 直接, 我們 ## 7 Topic 5 0.0767 豬隊友, 女兒, 小孩, 媽媽, 半夜, 餵奶 ## 8 Topic 8 0.0684 婆婆, 老公, 隊友, 幫忙, 婆家, 公婆 ## 9 Topic 11 0.0619 隊友, 護理, 醫院, 醫生, 陣痛, 真的 ## 10 Topic 7 0.0591 隊友, 醫生, 我們, 醫師, 診所, 檢查 ## 11 Topic 1 0.0442 問題, 知道, 媽媽, 發現, 有沒有, 一些 ## 12 Topic 3 0.0191 討論, 反應, 重點, 原因, 味道, 困擾 8.1.7 Other "],["introbookdown.html", "Chapter 9 Introduction", " Chapter 9 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 9. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 11. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 9.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 9.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 9.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 9.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2022) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["literature.html", "Chapter 10 Literature", " Chapter 10 Literature Here is a review of existing methods. "],["methods.html", "Chapter 11 Methods 11.1 math example", " Chapter 11 Methods We describe our methods in this chapter. Math can be added in body using usual syntax like this 11.1 math example \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] You can also use math in footnotes like this1. We will approximate standard error to 0.0272 where we mention \\(p = \\frac{a}{b}\\)↩︎ \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\]↩︎ "],["applications.html", "Chapter 12 Applications 12.1 Example one 12.2 Example two", " Chapter 12 Applications Some significant applications are demonstrated in this chapter. 12.1 Example one 12.2 Example two "],["final-words.html", "Chapter 13 Final Words", " Chapter 13 Final Words We have finished a nice book. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
